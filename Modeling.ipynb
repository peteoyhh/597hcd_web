{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cfc6ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'video_id', 'title', 'description', 'hashtags', 'channel',\n",
      "       'published_at', 'category_id', 'duration', 'definition', 'category',\n",
      "       'views', 'likes', 'comments', 'crawl_date', 'views_next',\n",
      "       'crawl_date:1', 'log_views', 'log_likes', 'log_comments', 'like_rate',\n",
      "       'comment_rate', 'pop_V1', 'pop_V2', 'pop_V3', 'pop_V4', 'pop_V5',\n",
      "       'views_1d', 'views_7d', 'views_30d', 'views_final'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./data/CLEANED_MERGED.csv\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2415cc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Ensure views, likes, comments exist, fill with 0 if missing\n",
    "for col in [\"views\", \"likes\", \"comments\"]:\n",
    "    if col not in df.columns:\n",
    "        df[col] = 0\n",
    "\n",
    "# Avoid division by zero\n",
    "df[\"views_safe\"] = df[\"views\"].replace(0, np.nan)\n",
    "\n",
    "df[\"like_rate\"] = df[\"likes\"] / df[\"views_safe\"]\n",
    "df[\"comment_rate\"] = df[\"comments\"] / df[\"views_safe\"]\n",
    "\n",
    "df[\"like_rate\"] = df[\"like_rate\"].fillna(0)\n",
    "df[\"comment_rate\"] = df[\"comment_rate\"].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e5ecf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"popularity_raw\"] = (\n",
    "    df[\"views\"] +\n",
    "    5 * df[\"likes\"] +\n",
    "    10 * df[\"comments\"] \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ba8da35",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_raw = df[\"popularity_raw\"].min()\n",
    "max_raw = df[\"popularity_raw\"].max()\n",
    "\n",
    "df[\"popularity_score\"] = (df[\"popularity_raw\"] - min_raw) / (max_raw - min_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "476fbc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# --- 1. Parse duration PTxxHxxMxxS â†’ seconds ---\n",
    "def parse_duration(duration_str):\n",
    "    if not isinstance(duration_str, str):\n",
    "        return np.nan\n",
    "    \n",
    "    pattern = r\"PT((?P<h>\\d+)H)?((?P<m>\\d+)M)?((?P<s>\\d+)S)?\"\n",
    "    m = re.match(pattern, duration_str)\n",
    "    if not m:\n",
    "        return np.nan\n",
    "    \n",
    "    h = int(m.group(\"h\")) if m.group(\"h\") else 0\n",
    "    m_ = int(m.group(\"m\")) if m.group(\"m\") else 0\n",
    "    s = int(m.group(\"s\")) if m.group(\"s\") else 0\n",
    "    return h * 3600 + m_ * 60 + s\n",
    "\n",
    "# --- 2. Feature Engineering ---\n",
    "df[\"duration_sec\"] = df[\"duration\"].apply(parse_duration)\n",
    "\n",
    "df[\"title_length\"] = df[\"title\"].fillna(\"\").apply(lambda x: len(x.split()))\n",
    "\n",
    "df[\"hashtag_count\"] = df[\"hashtags\"].fillna(\"\").apply(\n",
    "    lambda x: len([h for h in str(x).split() if h.startswith(\"#\")])\n",
    ")\n",
    "\n",
    "df[\"log_duration\"] = np.log1p(df[\"duration_sec\"])\n",
    "\n",
    "df[\"has_description\"] = df[\"description\"].fillna(\"\").apply(lambda x: 1 if len(x.strip()) > 0 else 0)\n",
    "\n",
    "df[[\"duration\", \"duration_sec\", \"log_duration\", \"title_length\", \"hashtag_count\", \"has_description\"]].head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df[\"popularity_label\"] = pd.qcut(\n",
    "    df[\"popularity_score\"],\n",
    "    q=3,\n",
    "    labels=[\"Low\", \"Medium\", \"High\"]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df.to_csv(\"./data/CLEANED_MERGED_with_popularity.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "01670249-b15b-4995-8611-79d54c8a23f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loaded source data\n",
      "  Source rows: 3901\n",
      "  Unique video_ids: 3901\n",
      "  Average rows per video: 1.0\n",
      "  Videos with multiple rows: 0\n",
      "\n",
      "Step 2: Split video_ids by category (1:9 ratio)\n",
      "  Assigned 3506 videos to train\n",
      "  Assigned 395 videos to test\n",
      "\n",
      "Step 3: Extract all rows from source_df based on video_id\n",
      "  Train rows before dedup: 3506\n",
      "  Test rows before dedup: 395\n",
      "\n",
      "Step 4: Deduplicate by crawl_date\n",
      "\n",
      "============================================================\n",
      "Final Split Summary\n",
      "============================================================\n",
      "Source: 3901 rows, 3901 unique video_ids\n",
      "Train: 3506 rows, 3506 unique video_ids (89.87% of rows)\n",
      "Test: 395 rows, 395 unique video_ids (10.13% of rows)\n",
      "Overlap video_ids: 0 (should be 0)\n",
      "\n",
      "Per-category summary:\n",
      "          category  total_videos  train_videos  test_videos video_test_pct  total_rows  train_rows  test_rows\n",
      "            comedy           448           403           45          10.0%         448         403         45\n",
      "         education           458           412           46          10.0%         458         412         46\n",
      "     entertainment           451           405           46          10.2%         451         405         46\n",
      "            gaming           424           381           43          10.1%         424         381         43\n",
      "       howto_style           113           101           12          10.6%         113         101         12\n",
      "             music           454           408           46          10.1%         454         408         46\n",
      "     news_politics           427           384           43          10.1%         427         384         43\n",
      "science_technology           272           244           28          10.3%         272         244         28\n",
      "            sports           435           391           44          10.1%         435         391         44\n",
      "       travel_vlog           419           377           42          10.0%         419         377         42\n",
      "\n",
      "âœ… Saved train_data.csv (3506 rows)\n",
      "âœ… Saved test_data.csv (395 rows)\n",
      "\n",
      "ðŸ“Š Video Statistics:\n",
      "  Average rows per video (train): 1.0\n",
      "  Average rows per video (test): 1.0\n",
      "  Videos with multiple rows (train): 0\n",
      "  Videos with multiple rows (test): 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "source_df = pd.read_csv(\"./data/CLEANED_MERGED_with_popularity.csv\")\n",
    "\n",
    "source_df = source_df.dropna(subset=[\"title\", \"popularity_label\"])\n",
    "source_df = source_df[source_df[\"title\"].str.strip() != \"\"]\n",
    "\n",
    "print(f\"Step 1: Loaded source data\")\n",
    "print(f\"  Source rows: {len(source_df)}\")\n",
    "print(f\"  Unique video_ids: {source_df['video_id'].nunique()}\")\n",
    "\n",
    "video_counts = source_df.groupby('video_id').size()\n",
    "print(f\"  Average rows per video: {video_counts.mean():.1f}\")\n",
    "print(f\"  Videos with multiple rows: {(video_counts > 1).sum()}\")\n",
    "\n",
    "train_video_ids = []\n",
    "test_video_ids = []\n",
    "\n",
    "print(f\"\\nStep 2: Split video_ids by category (1:9 ratio)\")\n",
    "np.random.seed(42)\n",
    "\n",
    "for category in source_df[\"category\"].dropna().unique():\n",
    "    cat_rows = source_df[source_df[\"category\"] == category]\n",
    "    cat_videos = cat_rows[\"video_id\"].unique().tolist()\n",
    "    \n",
    "    if len(cat_videos) == 0:\n",
    "        continue\n",
    "    \n",
    "    if len(cat_videos) == 1:\n",
    "        train_video_ids.append(cat_videos[0])\n",
    "        continue\n",
    "    \n",
    "    cat_train_vids, cat_test_vids = train_test_split(\n",
    "        cat_videos,\n",
    "        test_size=0.10,\n",
    "        random_state=42,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    train_video_ids.extend(cat_train_vids)\n",
    "    test_video_ids.extend(cat_test_vids)\n",
    "\n",
    "print(f\"  Assigned {len(train_video_ids)} videos to train\")\n",
    "print(f\"  Assigned {len(test_video_ids)} videos to test\")\n",
    "\n",
    "print(f\"\\nStep 3: Extract all rows from source_df based on video_id\")\n",
    "train_df = source_df[source_df[\"video_id\"].isin(train_video_ids)].copy()\n",
    "test_df = source_df[source_df[\"video_id\"].isin(test_video_ids)].copy()\n",
    "\n",
    "print(f\"  Train rows before dedup: {len(train_df)}\")\n",
    "print(f\"  Test rows before dedup: {len(test_df)}\")\n",
    "\n",
    "print(f\"\\nStep 4: Deduplicate by crawl_date\")\n",
    "if 'crawl_date' in train_df.columns:\n",
    "    train_df = train_df.drop_duplicates(subset=['video_id', 'crawl_date'], keep='first')\n",
    "\n",
    "if 'crawl_date' in test_df.columns:\n",
    "    test_df = test_df.drop_duplicates(subset=['video_id', 'crawl_date'], keep='first')\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Final Split Summary\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Source: {len(source_df)} rows, {source_df['video_id'].nunique()} unique video_ids\")\n",
    "print(f\"Train: {len(train_df)} rows, {train_df['video_id'].nunique()} unique video_ids ({len(train_df)/len(source_df)*100:.2f}% of rows)\")\n",
    "print(f\"Test: {len(test_df)} rows, {test_df['video_id'].nunique()} unique video_ids ({len(test_df)/len(source_df)*100:.2f}% of rows)\")\n",
    "\n",
    "overlap = set(train_video_ids) & set(test_video_ids)\n",
    "print(f\"Overlap video_ids: {len(overlap)} (should be 0)\")\n",
    "\n",
    "print(f\"\\nPer-category summary:\")\n",
    "category_summary = []\n",
    "for category in sorted(source_df[\"category\"].dropna().unique()):\n",
    "    cat_source = source_df[source_df[\"category\"] == category]\n",
    "    total_videos = cat_source[\"video_id\"].nunique()\n",
    "    total_rows = len(cat_source)\n",
    "    \n",
    "    train_rows = len(train_df[train_df[\"category\"] == category])\n",
    "    test_rows = len(test_df[test_df[\"category\"] == category])\n",
    "    \n",
    "    train_videos = train_df[train_df[\"category\"] == category][\"video_id\"].nunique()\n",
    "    test_videos = test_df[test_df[\"category\"] == category][\"video_id\"].nunique()\n",
    "    \n",
    "    video_split_pct = (test_videos / total_videos * 100) if total_videos > 0 else 0\n",
    "    \n",
    "    category_summary.append({\n",
    "        \"category\": category,\n",
    "        \"total_videos\": total_videos,\n",
    "        \"train_videos\": train_videos,\n",
    "        \"test_videos\": test_videos,\n",
    "        \"video_test_pct\": f\"{video_split_pct:.1f}%\",\n",
    "        \"total_rows\": total_rows,\n",
    "        \"train_rows\": train_rows,\n",
    "        \"test_rows\": test_rows\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(category_summary)\n",
    "display_cols = [\"category\", \"total_videos\", \"train_videos\", \"test_videos\", \"video_test_pct\", \"total_rows\", \"train_rows\", \"test_rows\"]\n",
    "print(summary_df[display_cols].to_string(index=False))\n",
    "\n",
    "train_df.to_csv(\"./data/train_data.csv\", index=False)\n",
    "test_df.to_csv(\"./data/test_data.csv\", index=False)\n",
    "\n",
    "print(f\"\\nâœ… Saved train_data.csv ({len(train_df)} rows)\")\n",
    "print(f\"âœ… Saved test_data.csv ({len(test_df)} rows)\")\n",
    "\n",
    "train_video_counts = train_df.groupby('video_id').size()\n",
    "test_video_counts = test_df.groupby('video_id').size()\n",
    "\n",
    "print(f\"\\nðŸ“Š Video Statistics:\")\n",
    "print(f\"  Average rows per video (train): {train_video_counts.mean():.1f}\")\n",
    "print(f\"  Average rows per video (test): {test_video_counts.mean():.1f}\")\n",
    "print(f\"  Videos with multiple rows (train): {(train_video_counts > 1).sum()}\")\n",
    "print(f\"  Videos with multiple rows (test): {(test_video_counts > 1).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6c7c5b25-1115-4c5f-8724-b8ac63be1477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data before: 20809 rows, 395 unique video_ids\n",
      "Source data: 205152 rows, 3902 unique video_ids\n",
      "\n",
      "Test data after: 20809 rows, 395 unique video_ids\n",
      "\n",
      "âœ… Updated test_data.csv (20809 rows)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "#This part is for the time -series study, Regulary Bert Classfier won't need this. \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "test_df = pd.read_csv(\"./data/test_data.csv\")\n",
    "source_df = pd.read_csv(\"./data/CLEANED_MERGED_with_popularity.csv\")\n",
    "\n",
    "# Remove all columns with _source suffix (leftover from merge operation)\n",
    "test_df = test_df[[col for col in test_df.columns if not col.endswith('_source')]]\n",
    "\n",
    "print(f\"Test data before: {len(test_df)} rows, {test_df['video_id'].nunique()} unique video_ids\")\n",
    "print(f\"Source data: {len(source_df)} rows, {source_df['video_id'].nunique()} unique video_ids\")\n",
    "\n",
    "test_video_ids = set(test_df[\"video_id\"].unique())\n",
    "source_rows_to_add = source_df[source_df[\"video_id\"].isin(test_video_ids)].copy()\n",
    "\n",
    "# Only keep columns that exist in test_df to avoid adding new columns\n",
    "existing_columns = test_df.columns.tolist()\n",
    "source_rows_to_add = source_rows_to_add[[col for col in existing_columns if col in source_rows_to_add.columns]]\n",
    "\n",
    "test_df = pd.concat([test_df, source_rows_to_add], ignore_index=True)\n",
    "\n",
    "if 'crawl_date' in test_df.columns:\n",
    "    test_df = test_df.drop_duplicates(subset=['video_id', 'crawl_date'], keep='first')\n",
    "\n",
    "print(f\"\\nTest data after: {len(test_df)} rows, {test_df['video_id'].nunique()} unique video_ids\")\n",
    "\n",
    "test_df = test_df.sort_values(by=['video_id', 'crawl_date'])\n",
    "\n",
    "test_df.to_csv(\"./data/test_data.csv\", index=False)\n",
    "print(f\"\\nâœ… Updated test_data.csv ({len(test_df)} rows)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e5853d38-1dba-45d5-8279-c51932319979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data before: 184342 rows, 3506 unique video_ids\n",
      "Source data: 205152 rows, 3902 unique video_ids\n",
      "\n",
      "Train data after: 184342 rows, 3506 unique video_ids\n",
      "\n",
      "âœ… Updated train_data.csv (184342 rows)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "#This part is for the time -series study, Regulary Bert Classfier won't need this. \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"./data/train_data.csv\")\n",
    "source_df = pd.read_csv(\"./data/CLEANED_MERGED_with_popularity.csv\")\n",
    "\n",
    "# Remove all columns with _source suffix (leftover from merge operation)\n",
    "train_df = train_df[[col for col in train_df.columns if not col.endswith('_source')]]\n",
    "\n",
    "print(f\"Train data before: {len(train_df)} rows, {train_df['video_id'].nunique()} unique video_ids\")\n",
    "print(f\"Source data: {len(source_df)} rows, {source_df['video_id'].nunique()} unique video_ids\")\n",
    "\n",
    "train_video_ids = set(train_df[\"video_id\"].unique())\n",
    "source_rows_to_add = source_df[source_df[\"video_id\"].isin(train_video_ids)].copy()\n",
    "\n",
    "# Only keep columns that exist in train_df to avoid adding new columns\n",
    "existing_columns = train_df.columns.tolist()\n",
    "source_rows_to_add = source_rows_to_add[[col for col in existing_columns if col in source_rows_to_add.columns]]\n",
    "\n",
    "train_df = pd.concat([train_df, source_rows_to_add], ignore_index=True)\n",
    "\n",
    "if 'crawl_date' in train_df.columns:\n",
    "    train_df = train_df.drop_duplicates(subset=['video_id', 'crawl_date'], keep='first')\n",
    "\n",
    "print(f\"\\nTrain data after: {len(train_df)} rows, {train_df['video_id'].nunique()} unique video_ids\")\n",
    "\n",
    "train_df = train_df.sort_values(by=['video_id', 'crawl_date'])\n",
    "\n",
    "train_df.to_csv(\"./data/train_data.csv\", index=False)\n",
    "print(f\"\\nâœ… Updated train_data.csv ({len(train_df)} rows)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4299e23b-8b16-44d7-8142-eef4f3ee0a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps (Apple Silicon GPU)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model moved to mps\n",
      "\n",
      "Class distribution:\n",
      "  Low (label 0): count=1522, weight=0.7678\n",
      "  Medium (label 1): count=1239, weight=0.9432\n",
      "  High (label 2): count=745, weight=1.5687\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1980' max='1980' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1980/1980 42:44, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.936800</td>\n",
       "      <td>0.878433</td>\n",
       "      <td>0.612536</td>\n",
       "      <td>0.602383</td>\n",
       "      <td>0.600828</td>\n",
       "      <td>0.612536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.798800</td>\n",
       "      <td>0.820429</td>\n",
       "      <td>0.623932</td>\n",
       "      <td>0.629916</td>\n",
       "      <td>0.640538</td>\n",
       "      <td>0.623932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.748700</td>\n",
       "      <td>0.814631</td>\n",
       "      <td>0.623932</td>\n",
       "      <td>0.632349</td>\n",
       "      <td>0.668324</td>\n",
       "      <td>0.623932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.803000</td>\n",
       "      <td>0.799838</td>\n",
       "      <td>0.643875</td>\n",
       "      <td>0.637009</td>\n",
       "      <td>0.636880</td>\n",
       "      <td>0.643875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.826000</td>\n",
       "      <td>0.794844</td>\n",
       "      <td>0.632479</td>\n",
       "      <td>0.640211</td>\n",
       "      <td>0.659589</td>\n",
       "      <td>0.632479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.689800</td>\n",
       "      <td>0.838471</td>\n",
       "      <td>0.663818</td>\n",
       "      <td>0.652828</td>\n",
       "      <td>0.651589</td>\n",
       "      <td>0.663818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.556400</td>\n",
       "      <td>0.872364</td>\n",
       "      <td>0.669516</td>\n",
       "      <td>0.668369</td>\n",
       "      <td>0.667798</td>\n",
       "      <td>0.669516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.536900</td>\n",
       "      <td>0.901496</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.663787</td>\n",
       "      <td>0.662511</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.475100</td>\n",
       "      <td>0.985692</td>\n",
       "      <td>0.655271</td>\n",
       "      <td>0.650614</td>\n",
       "      <td>0.647963</td>\n",
       "      <td>0.655271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.371700</td>\n",
       "      <td>1.010316</td>\n",
       "      <td>0.655271</td>\n",
       "      <td>0.648268</td>\n",
       "      <td>0.645434</td>\n",
       "      <td>0.655271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='11' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11/11 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results (on training split): {'eval_loss': 0.8723635077476501, 'eval_accuracy': 0.6695156695156695, 'eval_f1': 0.6683691863691863, 'eval_precision': 0.6677976770879038, 'eval_recall': 0.6695156695156695, 'eval_runtime': 8.7692, 'eval_samples_per_second': 40.026, 'eval_steps_per_second': 1.254, 'epoch': 10.0}\n",
      "Model saved to ./deberta_popularity_v3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "from transformers import (\n",
    "    DebertaV2Tokenizer,\n",
    "    DebertaV2ForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import Dataset\n",
    "\n",
    "# Check and set device (prefer MPS for M1, then CUDA, finally CPU)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(f\"Using device: {device} (Apple Silicon GPU)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"Using device: {device} (CPU only)\")\n",
    "\n",
    "train_df = pd.read_csv(\"./data/train_data.csv\")\n",
    "\n",
    "# (1) Enhanced text with structured features + (2) Category-aware with category_id\n",
    "def build_combined_text(row):\n",
    "    # Category ID factorization (category-aware modeling)\n",
    "    category_id = int(row[\"category_id\"]) if pd.notna(row[\"category_id\"]) else -1\n",
    "    category = str(row[\"category\"]) if pd.notna(row[\"category\"]) else \"Unknown\"\n",
    "    \n",
    "    # Structured features\n",
    "    title_len = int(row[\"title_length\"]) if pd.notna(row[\"title_length\"]) else 0\n",
    "    hashtag_count = int(row[\"hashtag_count\"]) if pd.notna(row[\"hashtag_count\"]) else 0\n",
    "    duration_sec = float(row[\"duration_sec\"]) if pd.notna(row[\"duration_sec\"]) else 0.0\n",
    "    log_duration = float(row[\"log_duration\"]) if pd.notna(row[\"log_duration\"]) else 0.0\n",
    "    has_desc = 1 if pd.notna(row[\"description\"]) and str(row[\"description\"]).strip() != \"\" else 0\n",
    "    \n",
    "    # Text fields\n",
    "    title = str(row[\"title\"]) if pd.notna(row[\"title\"]) else \"\"\n",
    "    hashtags = str(row[\"hashtags\"]) if pd.notna(row[\"hashtags\"]) and str(row[\"hashtags\"]).lower() != \"nan\" else \"\"\n",
    "    \n",
    "    # Combined text with structured features injected\n",
    "    combined = (\n",
    "        f\"CATEGORY_ID: {category_id}. \"\n",
    "        f\"CATEGORY: {category}. \"\n",
    "        f\"TITLE_LENGTH: {title_len} words. \"\n",
    "        f\"HASHTAG_COUNT: {hashtag_count}. \"\n",
    "        f\"DURATION_SEC: {duration_sec:.1f}. \"\n",
    "        f\"LOG_DURATION: {log_duration:.2f}. \"\n",
    "        f\"HAS_DESCRIPTION: {has_desc}. \"\n",
    "        f\"TITLE: {title}. \"\n",
    "        f\"HASHTAGS: {hashtags}\"\n",
    "    )\n",
    "    return combined\n",
    "\n",
    "train_df[\"text\"] = train_df.apply(build_combined_text, axis=1)\n",
    "\n",
    "label_mapping = {\"Low\": 0, \"Medium\": 1, \"High\": 2}\n",
    "train_df[\"label\"] = train_df[\"popularity_label\"].map(label_mapping)\n",
    "train_df = train_df[train_df[\"label\"].notna()]\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_df[\"text\"].tolist(),\n",
    "    train_df[\"label\"].tolist(),\n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    "    stratify=train_df[\"label\"]\n",
    ")\n",
    "\n",
    "model_name = \"microsoft/deberta-v3-base\"\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=256)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=256)\n",
    "\n",
    "class YouTubeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = YouTubeDataset(train_encodings, train_labels)\n",
    "val_dataset = YouTubeDataset(val_encodings, val_labels)\n",
    "\n",
    "num_labels = 3\n",
    "model = DebertaV2ForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "id2label = {0: \"Low\", 1: \"Medium\", 2: \"High\"}\n",
    "label2id = {\"Low\": 0, \"Medium\": 1, \"High\": 2}\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = label2id\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "print(f\"Model moved to {device}\")\n",
    "\n",
    "# (3) Compute class weights for handling class imbalance\n",
    "label_counts = train_df[\"label\"].value_counts().sort_index()\n",
    "num_classes = len(label_counts)\n",
    "total_samples = len(train_df)\n",
    "\n",
    "# Inverse frequency weighting: weight_c = N_total / (num_classes * count_c)\n",
    "class_weights = torch.tensor([\n",
    "    total_samples / (num_classes * label_counts.get(i, 1))\n",
    "    for i in range(num_classes)\n",
    "], dtype=torch.float32).to(device)\n",
    "\n",
    "print(f\"\\nClass distribution:\")\n",
    "for i, label_name in enumerate([\"Low\", \"Medium\", \"High\"]):\n",
    "    count = label_counts.get(i, 0)\n",
    "    weight = class_weights[i].item()\n",
    "    print(f\"  {label_name} (label {i}): count={count}, weight={weight:.4f}\")\n",
    "\n",
    "# (3) Custom Trainer with weighted loss + (4) Optional focal loss support\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, class_weights, use_focal_loss=False, focal_alpha=0.25, focal_gamma=2.0, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "        self.use_focal_loss = use_focal_loss\n",
    "        self.focal_alpha = focal_alpha\n",
    "        self.focal_gamma = focal_gamma\n",
    "        if not use_focal_loss:\n",
    "            # Standard weighted cross-entropy loss\n",
    "            self.loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        else:\n",
    "            # For focal loss, we'll compute it manually\n",
    "            self.loss_fn = None\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        # Handle num_items_in_batch and other kwargs that newer Trainer versions may pass\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        if not self.use_focal_loss:\n",
    "            # Standard weighted cross-entropy\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "        else:\n",
    "            # Focal loss: FL = -alpha * (1-p_t)^gamma * log(p_t)\n",
    "            ce_loss = nn.CrossEntropyLoss(weight=self.class_weights, reduction='none')(logits, labels)\n",
    "            pt = torch.exp(-ce_loss)  # p_t = exp(-CE_loss)\n",
    "            focal_loss = self.focal_alpha * (1 - pt) ** self.focal_gamma * ce_loss\n",
    "            loss = focal_loss.mean()\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Set training parameters based on device type\n",
    "use_fp16 = torch.cuda.is_available()  # MPS doesn't fully support fp16 yet, only enable on CUDA\n",
    "use_pin_memory = torch.cuda.is_available()  # MPS doesn't support pin_memory\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./deberta_popularity_v3\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=20,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    fp16=use_fp16,  # Only enable mixed precision training on CUDA\n",
    "    dataloader_pin_memory=use_pin_memory  # Only enable pin_memory on CUDA\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"weighted\", zero_division=0)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }\n",
    "\n",
    "# Use custom WeightedTrainer instead of default Trainer\n",
    "# Set use_focal_loss=True to enable focal loss (optional, default is weighted CE)\n",
    "USE_FOCAL_LOSS = False  # Set to True to enable focal loss\n",
    "trainer = WeightedTrainer(\n",
    "    class_weights=class_weights,\n",
    "    use_focal_loss=USE_FOCAL_LOSS,\n",
    "    focal_alpha=0.25,  # Focal loss alpha parameter\n",
    "    focal_gamma=2.0,   # Focal loss gamma parameter\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "val_results = trainer.evaluate()\n",
    "print(\"Validation Results (on training split):\", val_results)\n",
    "\n",
    "model.save_pretrained(\"./deberta_popularity_v3\")\n",
    "tokenizer.save_pretrained(\"./deberta_popularity_v3\")\n",
    "print(\"Model saved to ./deberta_popularity_v3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43b38f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_popularity(title, hashtags, category, model, tokenizer):\n",
    "    combined_text = f\"CATEGORY: {category}. TITLE: {title}. HASHTAGS: {hashtags}\"\n",
    "    \n",
    "    inputs = tokenizer(combined_text, truncation=True, padding=True, max_length=256, return_tensors=\"pt\")\n",
    "    \n",
    "    # Move inputs to the device where model is located\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Move to CPU first, then convert to numpy\n",
    "    probs = probabilities[0].cpu().numpy()\n",
    "    label_idx = np.argmax(probs)\n",
    "    \n",
    "    label_mapping = {0: \"Low\", 1: \"Medium\", 2: \"High\"}\n",
    "    predicted_label = label_mapping[label_idx]\n",
    "    \n",
    "    prob_dict = {\n",
    "        \"Low\": float(probs[0]),\n",
    "        \"Medium\": float(probs[1]),\n",
    "        \"High\": float(probs[2])\n",
    "    }\n",
    "    \n",
    "    return predicted_label, prob_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bf9b494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing predictions:\n",
      "\n",
      "Title: Top 10 Travel Destinations\n",
      "Category: Travel\n",
      "Predicted: Low\n",
      "Probabilities: Low=0.954, Medium=0.038, High=0.008\n",
      "\n",
      "Title: Funny Cat Compilation\n",
      "Category: Comedy\n",
      "Predicted: Medium\n",
      "Probabilities: Low=0.281, Medium=0.614, High=0.105\n",
      "\n",
      "Title: Cooking Recipe Tutorial\n",
      "Category: Howto & Style\n",
      "Predicted: Low\n",
      "Probabilities: Low=0.952, Medium=0.039, High=0.008\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import DebertaV2Tokenizer, DebertaV2ForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "model_path = \"./deberta_popularity_v3\"\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained(model_path)\n",
    "model = DebertaV2ForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "test_samples = [\n",
    "    (\"Top 10 Travel Destinations\", \"#travel #adventure\", \"Travel\"),\n",
    "    (\"Funny Cat Compilation\", \"#funny #comedy #cats\", \"Comedy\"),\n",
    "    (\"Cooking Recipe Tutorial\", \"#cooking #recipe\", \"Howto & Style\")\n",
    "]\n",
    "\n",
    "print(\"Testing predictions:\\n\")\n",
    "for title, hashtags, category in test_samples:\n",
    "    label, probs = predict_popularity(title, hashtags, category, model, tokenizer)\n",
    "    print(f\"Title: {title}\")\n",
    "    print(f\"Category: {category}\")\n",
    "    print(f\"Predicted: {label}\")\n",
    "    print(f\"Probabilities: Low={probs['Low']:.3f}, Medium={probs['Medium']:.3f}, High={probs['High']:.3f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "392a7881-651b-40a1-b44e-5c71cef22ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xuchen/myenv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Final Test Set Evaluation\n",
      "============================================================\n",
      "\n",
      "Accuracy: 0.6506\n",
      "Precision: 0.6535\n",
      "Recall: 0.6506\n",
      "F1-Score: 0.6511\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Low     0.7907    0.7816    0.7861       174\n",
      "      Medium     0.5256    0.5734    0.5485       143\n",
      "        High     0.5821    0.5000    0.5379        78\n",
      "\n",
      "    accuracy                         0.6506       395\n",
      "   macro avg     0.6328    0.6183    0.6242       395\n",
      "weighted avg     0.6535    0.6506    0.6511       395\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "from transformers import DebertaV2Tokenizer, DebertaV2ForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "test_df = pd.read_csv(\"./data/test_data.csv\")\n",
    "\n",
    "# Use the same enhanced text building function as training\n",
    "def build_combined_text(row):\n",
    "    # Category ID factorization (category-aware modeling)\n",
    "    category_id = int(row[\"category_id\"]) if pd.notna(row[\"category_id\"]) else -1\n",
    "    category = str(row[\"category\"]) if pd.notna(row[\"category\"]) else \"Unknown\"\n",
    "    \n",
    "    # Structured features\n",
    "    title_len = int(row[\"title_length\"]) if pd.notna(row[\"title_length\"]) else 0\n",
    "    hashtag_count = int(row[\"hashtag_count\"]) if pd.notna(row[\"hashtag_count\"]) else 0\n",
    "    duration_sec = float(row[\"duration_sec\"]) if pd.notna(row[\"duration_sec\"]) else 0.0\n",
    "    log_duration = float(row[\"log_duration\"]) if pd.notna(row[\"log_duration\"]) else 0.0\n",
    "    has_desc = 1 if pd.notna(row[\"description\"]) and str(row[\"description\"]).strip() != \"\" else 0\n",
    "    \n",
    "    # Text fields\n",
    "    title = str(row[\"title\"]) if pd.notna(row[\"title\"]) else \"\"\n",
    "    hashtags = str(row[\"hashtags\"]) if pd.notna(row[\"hashtags\"]) and str(row[\"hashtags\"]).lower() != \"nan\" else \"\"\n",
    "    \n",
    "    # Combined text with structured features injected\n",
    "    combined = (\n",
    "        f\"CATEGORY_ID: {category_id}. \"\n",
    "        f\"CATEGORY: {category}. \"\n",
    "        f\"TITLE_LENGTH: {title_len} words. \"\n",
    "        f\"HASHTAG_COUNT: {hashtag_count}. \"\n",
    "        f\"DURATION_SEC: {duration_sec:.1f}. \"\n",
    "        f\"LOG_DURATION: {log_duration:.2f}. \"\n",
    "        f\"HAS_DESCRIPTION: {has_desc}. \"\n",
    "        f\"TITLE: {title}. \"\n",
    "        f\"HASHTAGS: {hashtags}\"\n",
    "    )\n",
    "    return combined\n",
    "\n",
    "test_df[\"text\"] = test_df.apply(build_combined_text, axis=1)\n",
    "\n",
    "label_mapping = {\"Low\": 0, \"Medium\": 1, \"High\": 2}\n",
    "test_df[\"label\"] = test_df[\"popularity_label\"].map(label_mapping)\n",
    "test_df = test_df[test_df[\"label\"].notna()]\n",
    "\n",
    "test_texts = test_df[\"text\"].tolist()\n",
    "test_labels = test_df[\"label\"].tolist()\n",
    "\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained(\"./deberta_popularity_v3\")\n",
    "model = DebertaV2ForSequenceClassification.from_pretrained(\"./deberta_popularity_v3\")\n",
    "\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=256)\n",
    "\n",
    "class YouTubeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "test_dataset = YouTubeDataset(test_encodings, test_labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./deberta_popularity_v3\",\n",
    "    per_device_eval_batch_size=32,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args\n",
    ")\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "pred_labels = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Final Test Set Evaluation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "accuracy = accuracy_score(test_labels, pred_labels)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(test_labels, pred_labels, average=\"weighted\", zero_division=0)\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_labels, pred_labels, target_names=[\"Low\", \"Medium\", \"High\"], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ee15d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: MPS (Apple Silicon GPU)\n",
      "\n",
      "Loading data...\n",
      "  Original rows: 3506\n",
      "  Unique videos: 3506\n",
      "  After dropping missing targets: 3341 rows\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Hybrid Regression Model: DeBERTa Embeddings + Structured Features\n",
    "# ============================================================\n",
    "# Extract embeddings from fine-tuned DeBERTa model\n",
    "# Combine with structured features to predict log-transformed view growth\n",
    "# Compare RandomForest vs LightGBM and select best model per target\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DebertaV2Tokenizer, DebertaV2ForSequenceClassification\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Device selection: CUDA â†’ MPS â†’ CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using device: CUDA ({torch.cuda.get_device_name(0)})\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(f\"Using device: MPS (Apple Silicon GPU)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"Using device: CPU\")\n",
    "\n",
    "# Load data\n",
    "print(\"\\nLoading data...\")\n",
    "df = pd.read_csv(\"./data/train_data.csv\")\n",
    "print(f\"  Original rows: {len(df)}\")\n",
    "print(f\"  Unique videos: {df['video_id'].nunique()}\")\n",
    "\n",
    "# Drop rows with missing target values\n",
    "target_cols = [\"views_1d\", \"views_7d\", \"views_30d\"]\n",
    "df = df.dropna(subset=target_cols)\n",
    "print(f\"  After dropping missing targets: {len(df)} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0e37319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing missing features...\n",
      "  Computed: log_duration = log1p(duration_sec)\n",
      "\n",
      "Structured features ready: ['title_length', 'hashtag_count', 'duration_sec', 'log_duration', 'has_description', 'category_id']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/69/w45ft9yj2c94_h82xphjt5qw0000gn/T/ipykernel_18196/3549396446.py:81: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "/var/folders/69/w45ft9yj2c94_h82xphjt5qw0000gn/T/ipykernel_18196/3549396446.py:81: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Compute Missing Features\n",
    "# ============================================================\n",
    "\n",
    "def parse_duration_iso(duration_str):\n",
    "    \"\"\"Parse ISO 8601 duration format (PT39S, PT5M10S, PT1H2M3S) to seconds.\"\"\"\n",
    "    if pd.isna(duration_str) or duration_str == \"\":\n",
    "        return 0.0\n",
    "    \n",
    "    duration_str = str(duration_str).upper()\n",
    "    if not duration_str.startswith('PT'):\n",
    "        return 0.0\n",
    "    \n",
    "    pattern = r'PT(?:(\\d+)H)?(?:(\\d+)M)?(?:(\\d+)S)?'\n",
    "    match = re.match(pattern, duration_str)\n",
    "    if not match:\n",
    "        return 0.0\n",
    "    \n",
    "    hours = int(match.group(1) or 0)\n",
    "    minutes = int(match.group(2) or 0)\n",
    "    seconds = int(match.group(3) or 0)\n",
    "    \n",
    "    return hours * 3600 + minutes * 60 + seconds\n",
    "\n",
    "print(\"\\nComputing missing features...\")\n",
    "\n",
    "# Compute title_length if missing\n",
    "if \"title_length\" not in df.columns:\n",
    "    df[\"title_length\"] = df[\"title\"].apply(lambda x: len(str(x).split()) if pd.notna(x) else 0)\n",
    "    print(\"  Computed: title_length\")\n",
    "\n",
    "# Compute hashtag_count if missing\n",
    "if \"hashtag_count\" not in df.columns:\n",
    "    df[\"hashtag_count\"] = df[\"hashtags\"].apply(\n",
    "        lambda x: len(str(x).split()) if pd.notna(x) and str(x).lower() != \"nan\" else 0\n",
    "    )\n",
    "    print(\"  Computed: hashtag_count\")\n",
    "\n",
    "# Compute duration_sec if missing\n",
    "if \"duration_sec\" not in df.columns and \"duration\" in df.columns:\n",
    "    df[\"duration_sec\"] = df[\"duration\"].apply(parse_duration_iso)\n",
    "    print(\"  Computed: duration_sec from duration\")\n",
    "elif \"duration_sec\" not in df.columns:\n",
    "    df[\"duration_sec\"] = 0.0\n",
    "    print(\"  Warning: duration_sec not found, set to 0\")\n",
    "\n",
    "# Compute log_duration\n",
    "df[\"log_duration\"] = np.log1p(df[\"duration_sec\"])\n",
    "print(\"  Computed: log_duration = log1p(duration_sec)\")\n",
    "\n",
    "# Compute has_description\n",
    "if \"has_description\" not in df.columns:\n",
    "    df[\"has_description\"] = df[\"description\"].apply(\n",
    "        lambda x: 1 if pd.notna(x) and str(x).strip() != \"\" else 0\n",
    "    )\n",
    "    print(\"  Computed: has_description\")\n",
    "\n",
    "# Compute category_id if missing\n",
    "if \"category_id\" not in df.columns and \"category\" in df.columns:\n",
    "    df[\"category_id\"], _ = pd.factorize(df[\"category\"])\n",
    "    print(\"  Computed: category_id from category\")\n",
    "elif \"category_id\" not in df.columns:\n",
    "    df[\"category_id\"] = -1\n",
    "    print(\"  Warning: category_id not found, set to -1\")\n",
    "\n",
    "# Prepare structured features\n",
    "numeric_features = [\n",
    "    \"title_length\",\n",
    "    \"hashtag_count\",\n",
    "    \"duration_sec\",\n",
    "    \"log_duration\",\n",
    "    \"has_description\",\n",
    "    \"category_id\"\n",
    "]\n",
    "\n",
    "# Fill missing values\n",
    "for col in numeric_features:\n",
    "    if col in df.columns:\n",
    "        if df[col].isna().any():\n",
    "            if df[col].dtype in ['int64', 'float64']:\n",
    "                df[col].fillna(df[col].median(), inplace=True)\n",
    "            else:\n",
    "                df[col].fillna(0, inplace=True)\n",
    "\n",
    "print(f\"\\nStructured features ready: {numeric_features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ccc4c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target columns (log-transformed): ['log_views_1d', 'log_views_7d', 'log_views_30d']\n",
      "\n",
      "Train set: 3006 samples\n",
      "Test set: 335 samples\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Build Combined Text for DeBERTa (matching classification model format)\n",
    "# ============================================================\n",
    "\n",
    "def build_combined_text(row):\n",
    "    \"\"\"Build text input matching the classification model format.\"\"\"\n",
    "    category_id = int(row[\"category_id\"]) if pd.notna(row[\"category_id\"]) else -1\n",
    "    category = str(row[\"category\"]) if pd.notna(row[\"category\"]) else \"Unknown\"\n",
    "    title = str(row[\"title\"]) if pd.notna(row[\"title\"]) else \"\"\n",
    "    hashtags = str(row[\"hashtags\"]) if pd.notna(row[\"hashtags\"]) and str(row[\"hashtags\"]).lower() != \"nan\" else \"\"\n",
    "    \n",
    "    combined = f\"CATEGORY_ID: {category_id}. CATEGORY: {category}. TITLE: {title}. HASHTAGS: {hashtags}\"\n",
    "    return combined\n",
    "\n",
    "df[\"combined_text\"] = df.apply(build_combined_text, axis=1)\n",
    "\n",
    "# Prepare log-transformed targets\n",
    "df[\"log_views_1d\"] = np.log1p(df[\"views_1d\"])\n",
    "df[\"log_views_7d\"] = np.log1p(df[\"views_7d\"])\n",
    "df[\"log_views_30d\"] = np.log1p(df[\"views_30d\"])\n",
    "\n",
    "log_target_cols = [\"log_views_1d\", \"log_views_7d\", \"log_views_30d\"]\n",
    "print(f\"\\nTarget columns (log-transformed): {log_target_cols}\")\n",
    "\n",
    "# Train/Test Split (90% train, 10% test)\n",
    "train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "print(f\"\\nTrain set: {len(train_df)} samples\")\n",
    "print(f\"Test set: {len(test_df)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "805bc96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading fine-tuned DeBERTa model...\n",
      "  Model loaded and moved to mps\n",
      "\n",
      "Extracting DeBERTa embeddings...\n",
      "  Training set...\n",
      "    Shape: (3006, 768)\n",
      "  Test set...\n",
      "    Shape: (335, 768)\n",
      "\n",
      "Combining embeddings with structured features...\n",
      "  Train feature matrix: (3006, 774)\n",
      "  Test feature matrix: (335, 774)\n",
      "    (DeBERTa embeddings: 768 dims + 6 structured features)\n",
      "  Target matrix: (3006, 3)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Load Fine-tuned DeBERTa Model and Extract Embeddings\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nLoading fine-tuned DeBERTa model...\")\n",
    "model_path = \"./deberta_popularity_v3\"\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained(model_path)\n",
    "deberta_model = DebertaV2ForSequenceClassification.from_pretrained(model_path)\n",
    "deberta_model = deberta_model.to(device)\n",
    "deberta_model.eval()\n",
    "print(f\"  Model loaded and moved to {device}\")\n",
    "\n",
    "# Dataset class for text encoding\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=256):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts.iloc[idx])\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {k: v.squeeze() for k, v in encoding.items()}\n",
    "\n",
    "def extract_embeddings(texts, model, tokenizer, device, batch_size=32):\n",
    "    \"\"\"Extract CLS embeddings from DeBERTa model.\"\"\"\n",
    "    dataset = TextDataset(texts, tokenizer)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # Get hidden states\n",
    "            outputs = model.deberta(**batch, output_hidden_states=True)\n",
    "            hidden_states = outputs.hidden_states\n",
    "            \n",
    "            # Extract CLS token embedding (last layer, first token)\n",
    "            cls_embeddings = hidden_states[-1][:, 0, :].cpu().numpy()\n",
    "            embeddings.append(cls_embeddings)\n",
    "    \n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "print(\"\\nExtracting DeBERTa embeddings...\")\n",
    "print(\"  Training set...\")\n",
    "train_embeddings = extract_embeddings(train_df[\"combined_text\"], deberta_model, tokenizer, device)\n",
    "print(f\"    Shape: {train_embeddings.shape}\")\n",
    "\n",
    "print(\"  Test set...\")\n",
    "test_embeddings = extract_embeddings(test_df[\"combined_text\"], deberta_model, tokenizer, device)\n",
    "print(f\"    Shape: {test_embeddings.shape}\")\n",
    "\n",
    "# Combine embeddings with structured features\n",
    "print(\"\\nCombining embeddings with structured features...\")\n",
    "train_numeric = train_df[numeric_features].values\n",
    "test_numeric = test_df[numeric_features].values\n",
    "\n",
    "X_train = np.hstack([train_embeddings, train_numeric])\n",
    "X_test = np.hstack([test_embeddings, test_numeric])\n",
    "\n",
    "print(f\"  Train feature matrix: {X_train.shape}\")\n",
    "print(f\"  Test feature matrix: {X_test.shape}\")\n",
    "print(f\"    (DeBERTa embeddings: {train_embeddings.shape[1]} dims + {len(numeric_features)} structured features)\")\n",
    "\n",
    "# Prepare targets (log-transformed)\n",
    "Y_train = train_df[log_target_cols].values\n",
    "Y_test = test_df[log_target_cols].values\n",
    "print(f\"  Target matrix: {Y_train.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c0233679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Training Random Forest Regressors...\n",
      "================================================================================\n",
      "\n",
      "Training RF for log_views_1d...\n",
      "  RMSE: 1.3916, MAE: 1.0259, RÂ²: 0.6319\n",
      "\n",
      "Training RF for log_views_7d...\n",
      "  RMSE: 1.4679, MAE: 1.0884, RÂ²: 0.6131\n",
      "\n",
      "Training RF for log_views_30d...\n",
      "  RMSE: 1.5257, MAE: 1.1220, RÂ²: 0.5973\n",
      "\n",
      "Random Forest Average: RMSE=1.4618, MAE=1.0788, RÂ²=0.6141\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# MODEL 1: Random Forest Regressor (per target)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Training Random Forest Regressors...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "rf_models = {}\n",
    "rf_results = {}\n",
    "\n",
    "for i, target in enumerate(log_target_cols):\n",
    "    print(f\"\\nTraining RF for {target}...\")\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=500,\n",
    "        max_depth=None,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, Y_train[:, i])\n",
    "    rf_models[target] = model\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_true = Y_test[:, i]\n",
    "    \n",
    "    # Evaluate\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    rf_results[target] = {\n",
    "        \"RMSE\": rmse,\n",
    "        \"MAE\": mae,\n",
    "        \"RÂ²\": r2\n",
    "    }\n",
    "    \n",
    "    print(f\"  RMSE: {rmse:.4f}, MAE: {mae:.4f}, RÂ²: {r2:.4f}\")\n",
    "\n",
    "# Compute averages\n",
    "rf_results[\"avg\"] = {\n",
    "    \"RMSE\": np.mean([rf_results[t][\"RMSE\"] for t in log_target_cols]),\n",
    "    \"MAE\": np.mean([rf_results[t][\"MAE\"] for t in log_target_cols]),\n",
    "    \"RÂ²\": np.mean([rf_results[t][\"RÂ²\"] for t in log_target_cols])\n",
    "}\n",
    "\n",
    "print(f\"\\nRandom Forest Average: RMSE={rf_results['avg']['RMSE']:.4f}, \"\n",
    "      f\"MAE={rf_results['avg']['MAE']:.4f}, RÂ²={rf_results['avg']['RÂ²']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "00794ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LightGBM not available. Install with: pip install lightgbm\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# MODEL 2: LightGBM Regressor (per target)\n",
    "# ============================================================\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Training LightGBM Regressors...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    lgb_models = {}\n",
    "    lgb_results = {}\n",
    "    \n",
    "    for i, target in enumerate(log_target_cols):\n",
    "        print(f\"\\nTraining LightGBM for {target}...\")\n",
    "        model = LGBMRegressor(\n",
    "            n_estimators=500,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=8,\n",
    "            subsample=0.9,\n",
    "            colsample_bytree=0.9,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            verbose=-1\n",
    "        )\n",
    "        \n",
    "        model.fit(X_train, Y_train[:, i])\n",
    "        lgb_models[target] = model\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_true = Y_test[:, i]\n",
    "        \n",
    "        # Evaluate\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        \n",
    "        lgb_results[target] = {\n",
    "            \"RMSE\": rmse,\n",
    "            \"MAE\": mae,\n",
    "            \"RÂ²\": r2\n",
    "        }\n",
    "        \n",
    "        print(f\"  RMSE: {rmse:.4f}, MAE: {mae:.4f}, RÂ²: {r2:.4f}\")\n",
    "    \n",
    "    # Compute averages\n",
    "    lgb_results[\"avg\"] = {\n",
    "        \"RMSE\": np.mean([lgb_results[t][\"RMSE\"] for t in log_target_cols]),\n",
    "        \"MAE\": np.mean([lgb_results[t][\"MAE\"] for t in log_target_cols]),\n",
    "        \"RÂ²\": np.mean([lgb_results[t][\"RÂ²\"] for t in log_target_cols])\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nLightGBM Average: RMSE={lgb_results['avg']['RMSE']:.4f}, \"\n",
    "          f\"MAE={lgb_results['avg']['MAE']:.4f}, RÂ²={lgb_results['avg']['RÂ²']:.4f}\")\n",
    "    \n",
    "    lgb_available = True\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"\\nLightGBM not available. Install with: pip install lightgbm\")\n",
    "    lgb_available = False\n",
    "    lgb_models = {}\n",
    "    lgb_results = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bda00853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL COMPARISON\n",
      "================================================================================\n",
      "\n",
      "Per-Target Comparison:\n",
      "Target  RF_RMSE   RF_MAE    RF_RÂ²\n",
      "    1d 1.391634 1.025897 0.631947\n",
      "    7d 1.467926 1.088432 0.613137\n",
      "   30d 1.525723 1.121982 0.597305\n",
      "\n",
      "1d: Best model = RF (RÂ² = 0.6319)\n",
      "\n",
      "7d: Best model = RF (RÂ² = 0.6131)\n",
      "\n",
      "30d: Best model = RF (RÂ² = 0.5973)\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Random Forest - Avg RÂ²: 0.6141\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Model Comparison and Selection (Best Model Per Target)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create comparison DataFrames\n",
    "comparison_data = []\n",
    "\n",
    "for target in log_target_cols:\n",
    "    target_short = target.replace(\"log_views_\", \"\")\n",
    "    row = {\n",
    "        \"Target\": target_short,\n",
    "        \"RF_RMSE\": rf_results[target][\"RMSE\"],\n",
    "        \"RF_MAE\": rf_results[target][\"MAE\"],\n",
    "        \"RF_RÂ²\": rf_results[target][\"RÂ²\"]\n",
    "    }\n",
    "    if lgb_available:\n",
    "        row.update({\n",
    "            \"LGB_RMSE\": lgb_results[target][\"RMSE\"],\n",
    "            \"LGB_MAE\": lgb_results[target][\"MAE\"],\n",
    "            \"LGB_RÂ²\": lgb_results[target][\"RÂ²\"]\n",
    "        })\n",
    "    comparison_data.append(row)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nPer-Target Comparison:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Select best model for each target (based on RÂ²)\n",
    "best_models = {}\n",
    "best_model_names = {}\n",
    "\n",
    "for target in log_target_cols:\n",
    "    target_short = target.replace(\"log_views_\", \"\")\n",
    "    \n",
    "    rf_r2 = rf_results[target][\"RÂ²\"]\n",
    "    if lgb_available:\n",
    "        lgb_r2 = lgb_results[target][\"RÂ²\"]\n",
    "        if lgb_r2 > rf_r2:\n",
    "            best_models[target] = lgb_models[target]\n",
    "            best_model_names[target] = \"lgb\"\n",
    "        else:\n",
    "            best_models[target] = rf_models[target]\n",
    "            best_model_names[target] = \"rf\"\n",
    "    else:\n",
    "        best_models[target] = rf_models[target]\n",
    "        best_model_names[target] = \"rf\"\n",
    "    \n",
    "    print(f\"\\n{target_short}: Best model = {best_model_names[target].upper()} \"\n",
    "          f\"(RÂ² = {max(rf_r2, lgb_results[target]['RÂ²'] if lgb_available else rf_r2):.4f})\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Random Forest - Avg RÂ²: {rf_results['avg']['RÂ²']:.4f}\")\n",
    "if lgb_available:\n",
    "    print(f\"LightGBM - Avg RÂ²: {lgb_results['avg']['RÂ²']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bba25d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving best models...\n",
      "  âœ… 1d: RF â†’ ./models/rf_1d.pkl\n",
      "  âœ… 7d: RF â†’ ./models/rf_7d.pkl\n",
      "  âœ… 30d: RF â†’ ./models/rf_30d.pkl\n",
      "\n",
      "âœ… Metadata saved to: ./models/growth_model_metadata.json\n",
      "\n",
      "================================================================================\n",
      "Hybrid Regression Framework - Complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Save Best Models Per Target\n",
    "# ============================================================\n",
    "\n",
    "os.makedirs(\"./models\", exist_ok=True)\n",
    "\n",
    "print(\"\\nSaving best models...\")\n",
    "\n",
    "for target in log_target_cols:\n",
    "    target_short = target.replace(\"log_views_\", \"\")\n",
    "    model_name = best_model_names[target]\n",
    "    model_path = f\"./models/{model_name}_{target_short}.pkl\"\n",
    "    \n",
    "    joblib.dump(best_models[target], model_path)\n",
    "    print(f\"  âœ… {target_short}: {model_name.upper()} â†’ {model_path}\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    \"embedding_dim\": train_embeddings.shape[1],\n",
    "    \"numeric_features\": numeric_features,\n",
    "    \"target_cols\": log_target_cols,\n",
    "    \"best_models\": {target.replace(\"log_views_\", \"\"): best_model_names[target] for target in log_target_cols},\n",
    "    \"performance\": {\n",
    "        \"rf_avg_r2\": float(rf_results[\"avg\"][\"RÂ²\"]),\n",
    "        \"rf_avg_rmse\": float(rf_results[\"avg\"][\"RMSE\"]),\n",
    "        \"rf_avg_mae\": float(rf_results[\"avg\"][\"MAE\"])\n",
    "    },\n",
    "    \"train_size\": int(len(X_train)),\n",
    "    \"test_size\": int(len(X_test))\n",
    "}\n",
    "\n",
    "if lgb_available:\n",
    "    metadata[\"performance\"].update({\n",
    "        \"lgb_avg_r2\": float(lgb_results[\"avg\"][\"RÂ²\"]),\n",
    "        \"lgb_avg_rmse\": float(lgb_results[\"avg\"][\"RMSE\"]),\n",
    "        \"lgb_avg_mae\": float(lgb_results[\"avg\"][\"MAE\"])\n",
    "    })\n",
    "\n",
    "metadata_path = \"./models/growth_model_metadata.json\"\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"\\nâœ… Metadata saved to: {metadata_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Hybrid Regression Framework - Complete!\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a8113f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abee81cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c71c4e-d278-45a7-b9d2-95f85b4fba26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03278cb-aa20-4069-8dd6-8cb4a2c8dbe5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}